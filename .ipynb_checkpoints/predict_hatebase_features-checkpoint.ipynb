{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file tests the one layer NN vs a direct linear classifier using GloVe vectors to predict the 8 hatebase features. We define a new cost function that is similar to AUC but deals with the 8 features separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_custom_models import OneLayerNN, SoftmaxClassifier\n",
    "from utility import train_and_eval_auc, HATEBASE_FIELDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/hatebase\"\n",
    "EMBEDDING_SIZE = 100\n",
    "STATE_SIZE = 100\n",
    "GLOVE_SIZE = 1193514\n",
    "GLOVE_PATH = \"data/glove/glove.twitter.27B.%dd.txt\" % EMBEDDING_SIZE\n",
    "\n",
    "EMBED_PATH = \"data/hatebase/embeddings.%dd.dat\" % EMBEDDING_SIZE\n",
    "HIDDEN_EMBED_PATH = \"data/hatebase/embeddings.hidden.%dd.dat\" % EMBEDDING_SIZE\n",
    "HB_PATH = \"data/hatebase/lexicon.csv\"\n",
    "VOCAB_PATH = \"data/hatebase/vocab.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embed_path, vocab, force=False):\n",
    "    if not os.path.exists(embed_path):\n",
    "        hb_vecs = np.zeros((len(vocab), EMBEDDING_SIZE))\n",
    "        with open(GLOVE_PATH, 'r') as fh:\n",
    "            found = []\n",
    "            for line in tqdm(fh, total=GLOVE_SIZE):\n",
    "                array = line.strip().split(\" \")\n",
    "                word = array[0]\n",
    "                if word in vocab:\n",
    "                    idx = vocab[word]\n",
    "                    found.append(idx)\n",
    "                    vector = list(map(float, array[1:]))\n",
    "                    hb_vecs[idx, :] = vector\n",
    "            # words not found are set to random values\n",
    "            unfound = list(set(vocab.values()) - set(found))\n",
    "            for i in unfound:\n",
    "                hb_vecs[i, :] = np.random.randn(EMBEDDING_SIZE)\n",
    "                \n",
    "        hb_vecs = pd.DataFrame(hb_vecs)\n",
    "        hb_vecs.to_csv(embed_path, header = False, index = False)\n",
    "        return hb_vecs\n",
    "\n",
    "    with open(embed_path, 'rb') as embed_path:\n",
    "        data_x = pd.read_csv( embed_path, header = None, quoting = 0, dtype = np.float32 )\n",
    "        return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# grab the data\n",
    "hatebase_data = pd.read_csv( HB_PATH, header = 0, index_col = 0, quoting = 0, \n",
    "                                dtype = HATEBASE_FIELDS, usecols = range(9) )\n",
    "vocab = dict([(x, y) for (y, x) in enumerate(hatebase_data.index)])\n",
    "hatebase_embeddings = load_embeddings(EMBED_PATH, vocab, True)\n",
    "\n",
    "train_i, test_i = train_test_split( np.arange( len( hatebase_embeddings )), train_size = 0.8, random_state = 44 )\n",
    "train_x = hatebase_embeddings.iloc[train_i]\n",
    "test_x = hatebase_embeddings.iloc[test_i]\n",
    "train_y = hatebase_data.iloc[train_i]\n",
    "test_y = hatebase_data.iloc[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_jaccard( train_x, train_y, test_x, test_y, model ):\n",
    "    model.fit( train_x, train_y )\n",
    "    p = model.predict( test_x )\n",
    "    #print p\n",
    "    p = (p >= 0.5).astype(float)\n",
    "    total = sum([jaccard_similarity_score(y_true, y_pred) for y_true, y_pred in zip(test_y, p)])\n",
    "    print \"Total Jaccard similarity:\", total/len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_auc( train_x, train_y, test_x, test_y, model ):\n",
    "    model.fit( train_x, train_y )\n",
    "    p = model.predict_proba( test_x )\n",
    "    p = p[:,1] if p.shape[1] > 1 else p[:,0]\n",
    "\n",
    "    auc = AUC( test_y, p )\n",
    "    print \"AUC:\", auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.161815926433 "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "nn = OneLayerNN()\n",
    "nn.fit( hatebase_embeddings, hatebase_data )\n",
    "hidden_states = nn.return_hidden_states( hatebase_embeddings )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6   \\\n",
      "0    0.456425  0.414027  0.604445  0.572907  0.432509  0.439992  0.494656   \n",
      "1    0.575999  0.455576  0.908471  0.449966  0.684535  0.407860  0.488815   \n",
      "2    0.493667  0.476726  0.282094  0.735881  0.467950  0.133803  0.946229   \n",
      "3    0.311967  0.619841  0.234967  0.085983  0.134568  0.138706  0.719238   \n",
      "4    0.529106  0.495026  0.551225  0.625327  0.552845  0.330481  0.562362   \n",
      "5    0.286733  0.408641  0.223853  0.677007  0.429127  0.423491  0.435312   \n",
      "6    0.662635  0.467512  0.368308  0.603700  0.416271  0.715217  0.264777   \n",
      "7    0.527670  0.726104  0.595309  0.514192  0.426509  0.479006  0.489968   \n",
      "8    0.589656  0.618188  0.280485  0.339683  0.666604  0.244354  0.482003   \n",
      "9    0.374517  0.616019  0.730836  0.519205  0.365687  0.601393  0.548884   \n",
      "10   0.552217  0.590203  0.656959  0.602282  0.625810  0.339995  0.580408   \n",
      "11   0.180886  0.732642  0.722418  0.650592  0.513026  0.448571  0.597469   \n",
      "12   0.629776  0.559387  0.630849  0.530412  0.312349  0.053554  0.317784   \n",
      "13   0.671490  0.596646  0.839648  0.693325  0.623206  0.626383  0.157079   \n",
      "14   0.233603  0.463202  0.529914  0.490255  0.617144  0.799137  0.724689   \n",
      "15   0.516945  0.641808  0.653289  0.627042  0.641213  0.641262  0.242815   \n",
      "16   0.857790  0.186776  0.819371  0.829011  0.641446  0.661695  0.374716   \n",
      "17   0.771058  0.758019  0.530423  0.510167  0.420659  0.184065  0.397509   \n",
      "18   0.657438  0.344709  0.762127  0.697969  0.799157  0.432614  0.193356   \n",
      "19   0.181877  0.185478  0.177715  0.287370  0.550503  0.143052  0.608416   \n",
      "20   0.299221  0.191011  0.263656  0.438367  0.612793  0.258435  0.570773   \n",
      "21   0.405565  0.308847  0.147024  0.637616  0.428154  0.396504  0.361119   \n",
      "22   0.596862  0.384706  0.467620  0.676836  0.402930  0.541492  0.527145   \n",
      "23   0.572780  0.309888  0.394878  0.438329  0.400318  0.448894  0.402438   \n",
      "24   0.595407  0.565107  0.597804  0.348263  0.745720  0.522462  0.493896   \n",
      "25   0.818079  0.247611  0.518633  0.795776  0.821899  0.849550  0.306975   \n",
      "26   0.819048  0.123976  0.811066  0.218222  0.246396  0.044491  0.323998   \n",
      "27   0.497580  0.375698  0.465015  0.609925  0.516713  0.503552  0.642396   \n",
      "28   0.462912  0.683977  0.147732  0.593531  0.268752  0.206454  0.588886   \n",
      "29   0.465805  0.303430  0.310470  0.554408  0.452752  0.252354  0.610176   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "552  0.220535  0.503111  0.677011  0.588949  0.918065  0.866125  0.371312   \n",
      "553  0.502249  0.266060  0.418626  0.781961  0.645038  0.653860  0.479696   \n",
      "554  0.418466  0.355341  0.333923  0.547782  0.445539  0.212525  0.619021   \n",
      "555  0.209487  0.922497  0.537784  0.762513  0.784475  0.455426  0.841292   \n",
      "556  0.613675  0.440231  0.690775  0.376278  0.458942  0.458034  0.481673   \n",
      "557  0.539136  0.263073  0.447873  0.448314  0.504583  0.225594  0.744595   \n",
      "558  0.656714  0.438769  0.474236  0.407354  0.725760  0.614559  0.614059   \n",
      "559  0.453420  0.249071  0.477266  0.397628  0.335691  0.509086  0.475069   \n",
      "560  0.657226  0.414861  0.527812  0.515257  0.670613  0.465522  0.590133   \n",
      "561  0.618810  0.740015  0.246900  0.152311  0.511013  0.247228  0.458667   \n",
      "562  0.912813  0.475011  0.505116  0.165477  0.566204  0.166313  0.317133   \n",
      "563  0.329612  0.421277  0.296839  0.510381  0.559958  0.279991  0.505727   \n",
      "564  0.171726  0.244817  0.867966  0.658719  0.513405  0.372088  0.684729   \n",
      "565  0.612019  0.393585  0.622883  0.366924  0.484159  0.605453  0.353477   \n",
      "566  0.336640  0.326838  0.344905  0.595147  0.557566  0.318435  0.518252   \n",
      "567  0.458828  0.459780  0.595994  0.597378  0.616149  0.391267  0.562865   \n",
      "568  0.161282  0.678225  0.415385  0.380225  0.096152  0.324611  0.655007   \n",
      "569  0.574114  0.503583  0.214126  0.674912  0.340877  0.412332  0.570472   \n",
      "570  0.576392  0.241693  0.556377  0.302048  0.597913  0.388563  0.950358   \n",
      "571  0.489461  0.471472  0.545238  0.277954  0.584014  0.592066  0.388823   \n",
      "572  0.332713  0.202424  0.146506  0.558953  0.351916  0.234181  0.485550   \n",
      "573  0.270137  0.406175  0.329022  0.554681  0.272510  0.361789  0.607141   \n",
      "574  0.379703  0.263773  0.327262  0.450520  0.359421  0.339291  0.527377   \n",
      "575  0.912257  0.311839  0.366548  0.338988  0.442095  0.293350  0.250296   \n",
      "576  0.501995  0.282504  0.762024  0.464297  0.687184  0.615827  0.198786   \n",
      "577  0.741329  0.608376  0.515271  0.802380  0.701270  0.175117  0.167944   \n",
      "578  0.145385  0.739389  0.748478  0.557327  0.523183  0.266838  0.479091   \n",
      "579  0.638376  0.412160  0.669671  0.614422  0.547440  0.372565  0.501963   \n",
      "580  0.624115  0.220801  0.434815  0.211677  0.146402  0.543590  0.884538   \n",
      "581  0.771985  0.597795  0.257777  0.399439  0.647060  0.222633  0.473482   \n",
      "\n",
      "           7         8         9     ...           40        41        42  \\\n",
      "0    0.408126  0.548632  0.290534    ...     0.390321  0.207500  0.641884   \n",
      "1    0.311170  0.225458  0.885397    ...     0.195115  0.614023  0.875873   \n",
      "2    0.812132  0.157029  0.318078    ...     0.462184  0.220375  0.706491   \n",
      "3    0.942404  0.458797  0.748808    ...     0.621683  0.453654  0.784608   \n",
      "4    0.443863  0.383446  0.606148    ...     0.569280  0.203120  0.483164   \n",
      "5    0.547433  0.571679  0.379004    ...     0.656374  0.544090  0.716005   \n",
      "6    0.620360  0.471823  0.490918    ...     0.661291  0.317785  0.451808   \n",
      "7    0.229116  0.448688  0.363577    ...     0.306937  0.379515  0.427454   \n",
      "8    0.584530  0.362215  0.662409    ...     0.456534  0.544381  0.259148   \n",
      "9    0.331579  0.735784  0.508112    ...     0.409924  0.250477  0.323186   \n",
      "10   0.466887  0.543524  0.449456    ...     0.536548  0.362017  0.527282   \n",
      "11   0.624788  0.723340  0.194694    ...     0.590310  0.337231  0.830994   \n",
      "12   0.634553  0.238637  0.266678    ...     0.194953  0.269096  0.483991   \n",
      "13   0.718678  0.239916  0.196464    ...     0.505662  0.674412  0.781248   \n",
      "14   0.648552  0.937357  0.521678    ...     0.738264  0.695009  0.623805   \n",
      "15   0.412471  0.826769  0.624995    ...     0.661649  0.478451  0.390691   \n",
      "16   0.394882  0.337833  0.082779    ...     0.623762  0.187035  0.804770   \n",
      "17   0.493233  0.300743  0.875379    ...     0.465840  0.796557  0.888956   \n",
      "18   0.078533  0.934863  0.748146    ...     0.227375  0.158898  0.509157   \n",
      "19   0.651127  0.708310  0.367713    ...     0.575179  0.205089  0.474519   \n",
      "20   0.629383  0.723935  0.332446    ...     0.589911  0.216042  0.521671   \n",
      "21   0.527878  0.502524  0.377154    ...     0.308867  0.350690  0.480082   \n",
      "22   0.461302  0.567009  0.486116    ...     0.320837  0.194279  0.489281   \n",
      "23   0.518595  0.538199  0.364026    ...     0.472277  0.200313  0.598104   \n",
      "24   0.408420  0.638687  0.493941    ...     0.511305  0.670478  0.660518   \n",
      "25   0.228777  0.557579  0.297879    ...     0.212071  0.437492  0.815262   \n",
      "26   0.431262  0.333962  0.248082    ...     0.780791  0.939757  0.792595   \n",
      "27   0.413124  0.526953  0.397196    ...     0.330505  0.263521  0.524513   \n",
      "28   0.420323  0.199902  0.417543    ...     0.221756  0.316438  0.722001   \n",
      "29   0.481863  0.363635  0.455964    ...     0.563436  0.223021  0.639951   \n",
      "..        ...       ...       ...    ...          ...       ...       ...   \n",
      "552  0.610470  0.129359  0.892666    ...     0.392679  0.557957  0.334521   \n",
      "553  0.346756  0.652386  0.466570    ...     0.567715  0.403037  0.279440   \n",
      "554  0.480915  0.443857  0.612219    ...     0.548767  0.422795  0.584879   \n",
      "555  0.525380  0.638008  0.208897    ...     0.604337  0.403600  0.704706   \n",
      "556  0.376934  0.483302  0.352851    ...     0.461338  0.197258  0.419230   \n",
      "557  0.477793  0.731796  0.418959    ...     0.456136  0.283351  0.453439   \n",
      "558  0.481066  0.438301  0.527276    ...     0.563153  0.171783  0.253173   \n",
      "559  0.636923  0.684420  0.515155    ...     0.511485  0.202297  0.496931   \n",
      "560  0.397205  0.471043  0.586686    ...     0.432472  0.216976  0.532186   \n",
      "561  0.736239  0.504193  0.575641    ...     0.298650  0.248718  0.382845   \n",
      "562  0.262568  0.497562  0.243283    ...     0.822425  0.894351  0.699460   \n",
      "563  0.435068  0.574261  0.598397    ...     0.679936  0.401260  0.701322   \n",
      "564  0.324094  0.852378  0.764203    ...     0.293083  0.439683  0.422424   \n",
      "565  0.365328  0.687646  0.485891    ...     0.540337  0.314082  0.472282   \n",
      "566  0.649983  0.444827  0.819314    ...     0.696927  0.438019  0.484442   \n",
      "567  0.401317  0.626569  0.489012    ...     0.417744  0.367501  0.576849   \n",
      "568  0.144989  0.399733  0.724146    ...     0.290446  0.840146  0.547055   \n",
      "569  0.424920  0.417983  0.246334    ...     0.587639  0.209018  0.491310   \n",
      "570  0.409255  0.737358  0.344750    ...     0.243846  0.643288  0.809244   \n",
      "571  0.471030  0.676242  0.505530    ...     0.686531  0.404998  0.243430   \n",
      "572  0.578127  0.638184  0.529863    ...     0.494571  0.239644  0.515371   \n",
      "573  0.699916  0.472327  0.513772    ...     0.406684  0.323006  0.546720   \n",
      "574  0.466725  0.539498  0.488872    ...     0.469027  0.391589  0.724703   \n",
      "575  0.117113  0.584929  0.222212    ...     0.163715  0.617409  0.751427   \n",
      "576  0.393004  0.558789  0.591054    ...     0.175370  0.404822  0.348621   \n",
      "577  0.253651  0.443876  0.388191    ...     0.818086  0.731144  0.516235   \n",
      "578  0.063825  0.538883  0.552481    ...     0.438558  0.608795  0.532593   \n",
      "579  0.311891  0.386332  0.407473    ...     0.233515  0.300713  0.505925   \n",
      "580  0.562387  0.317030  0.470898    ...     0.104555  0.601196  0.408269   \n",
      "581  0.242655  0.698071  0.408145    ...     0.520410  0.610395  0.955707   \n",
      "\n",
      "           43        44        45        46        47        48        49  \n",
      "0    0.557099  0.483287  0.407877  0.807542  0.572776  0.694788  0.249705  \n",
      "1    0.367574  0.786041  0.725911  0.328243  0.320677  0.860950  0.509128  \n",
      "2    0.266503  0.293650  0.039841  0.867892  0.764552  0.869027  0.687928  \n",
      "3    0.120740  0.579393  0.886743  0.648282  0.643345  0.536317  0.520087  \n",
      "4    0.398436  0.614738  0.208607  0.683339  0.579072  0.497921  0.404887  \n",
      "5    0.446317  0.244523  0.261083  0.345015  0.606574  0.753033  0.522852  \n",
      "6    0.715506  0.404019  0.508922  0.302898  0.744356  0.417399  0.297712  \n",
      "7    0.559517  0.301374  0.429582  0.678425  0.749176  0.728946  0.349422  \n",
      "8    0.268732  0.349918  0.628573  0.530845  0.511842  0.752019  0.458863  \n",
      "9    0.511316  0.638877  0.408192  0.585546  0.518716  0.547624  0.335152  \n",
      "10   0.555372  0.374243  0.340116  0.736918  0.596065  0.501693  0.339147  \n",
      "11   0.864152  0.876562  0.539327  0.801831  0.865948  0.489310  0.442069  \n",
      "12   0.279161  0.924268  0.421489  0.154569  0.871809  0.454941  0.834814  \n",
      "13   0.706824  0.378149  0.851828  0.535246  0.300565  0.597443  0.208137  \n",
      "14   0.537721  0.798073  0.510060  0.731769  0.207943  0.892559  0.308230  \n",
      "15   0.343361  0.447953  0.339427  0.764002  0.288666  0.270715  0.162985  \n",
      "16   0.697679  0.321148  0.762777  0.090483  0.885672  0.824336  0.627012  \n",
      "17   0.568143  0.740016  0.346172  0.220595  0.520757  0.778089  0.572861  \n",
      "18   0.291774  0.375502  0.783842  0.758775  0.090335  0.175857  0.485818  \n",
      "19   0.266255  0.500654  0.431874  0.558824  0.394585  0.466911  0.647471  \n",
      "20   0.311019  0.577490  0.413855  0.525003  0.337170  0.404178  0.519780  \n",
      "21   0.381461  0.479970  0.330262  0.537146  0.703274  0.596220  0.467452  \n",
      "22   0.551617  0.468216  0.575792  0.509126  0.505131  0.505805  0.412105  \n",
      "23   0.425883  0.406499  0.521264  0.425012  0.647296  0.473667  0.281345  \n",
      "24   0.663752  0.365869  0.370520  0.546890  0.295682  0.560029  0.432398  \n",
      "25   0.172227  0.195176  0.932246  0.372892  0.346736  0.708590  0.795342  \n",
      "26   0.429954  0.722244  0.449856  0.834602  0.915065  0.938281  0.298376  \n",
      "27   0.581282  0.553258  0.387195  0.766742  0.655556  0.544184  0.399999  \n",
      "28   0.231898  0.448423  0.520784  0.251692  0.783395  0.844241  0.475363  \n",
      "29   0.345651  0.483664  0.226708  0.611105  0.467014  0.548076  0.518369  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "552  0.275854  0.392931  0.131929  0.288957  0.386477  0.650456  0.225173  \n",
      "553  0.633802  0.384170  0.279784  0.562664  0.615002  0.621866  0.418082  \n",
      "554  0.223804  0.420021  0.426768  0.589134  0.346511  0.741128  0.381101  \n",
      "555  0.248554  0.186894  0.451327  0.484381  0.578760  0.777578  0.886211  \n",
      "556  0.506525  0.566896  0.593270  0.661031  0.666292  0.661558  0.337064  \n",
      "557  0.839032  0.801662  0.110045  0.740989  0.705958  0.538743  0.603038  \n",
      "558  0.315884  0.333572  0.691602  0.785216  0.386475  0.358749  0.586145  \n",
      "559  0.611818  0.539987  0.434127  0.388296  0.574698  0.488568  0.270833  \n",
      "560  0.407409  0.566225  0.233662  0.440570  0.547109  0.671842  0.623801  \n",
      "561  0.247484  0.558816  0.785562  0.433418  0.192150  0.604762  0.473511  \n",
      "562  0.340202  0.487345  0.947572  0.147924  0.281633  0.536938  0.751183  \n",
      "563  0.502123  0.499722  0.434586  0.437241  0.451866  0.697943  0.462162  \n",
      "564  0.344318  0.397264  0.723606  0.638634  0.337532  0.448181  0.460704  \n",
      "565  0.532164  0.450469  0.589934  0.581009  0.537351  0.408429  0.606065  \n",
      "566  0.463275  0.779680  0.283168  0.492826  0.551111  0.622284  0.284032  \n",
      "567  0.460383  0.475977  0.457561  0.861105  0.528822  0.563028  0.306680  \n",
      "568  0.396795  0.069076  0.257178  0.838779  0.581527  0.805095  0.267076  \n",
      "569  0.561298  0.407991  0.296139  0.660329  0.661147  0.675022  0.497821  \n",
      "570  0.909146  0.223485  0.561710  0.395292  0.793484  0.386601  0.461994  \n",
      "571  0.660666  0.594062  0.761269  0.678277  0.535981  0.405110  0.548566  \n",
      "572  0.589658  0.700697  0.185100  0.513443  0.626473  0.609704  0.687562  \n",
      "573  0.446561  0.425802  0.270584  0.518737  0.509063  0.581122  0.522295  \n",
      "574  0.335840  0.482398  0.214570  0.356733  0.511409  0.806477  0.574347  \n",
      "575  0.813688  0.347882  0.889291  0.402995  0.562457  0.218330  0.205673  \n",
      "576  0.502101  0.610264  0.169036  0.180269  0.046872  0.206196  0.899062  \n",
      "577  0.476520  0.325316  0.211719  0.726684  0.708401  0.476218  0.576665  \n",
      "578  0.752356  0.044504  0.321913  0.782703  0.156928  0.355580  0.270584  \n",
      "579  0.634243  0.482655  0.294761  0.868799  0.653255  0.659044  0.419353  \n",
      "580  0.765604  0.821754  0.056182  0.517126  0.866074  0.949864  0.851921  \n",
      "581  0.318651  0.753913  0.697935  0.192296  0.688037  0.681831  0.835128  \n",
      "\n",
      "[582 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# write hidden states\n",
    "hidden_states = pd.DataFrame(hidden_states)\n",
    "hidden_states.to_csv(HIDDEN_EMBED_PATH, header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH, mode=\"wb\") as vocab_file:\n",
    "    for w in hatebase_data.index.values:\n",
    "        vocab_file.write(w + b\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.0911179706454 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Jaccard similarity: 0.793650793651\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "nn = OneLayerNN()\n",
    "total_jaccard( train_x, train_y.iloc[:,:7], test_x, test_y.iloc[:,:7].values, nn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about_class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.184165582061 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.472934472934\n",
      "about_ethnicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.0597166158259 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.564257028112\n",
      "about_sexual_orientation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.0233292710036 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.474358974359\n",
      "about_religion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.160992875695 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.525017618041\n",
      "about_disability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.168781414628 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.554943373125\n",
      "about_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.0206541772932 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.585034013605\n",
      "about_nationality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.0504829958081 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.578034682081\n",
      "offensiveness\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-36768bb59024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_and_eval_auc\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneLayerNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-e847b1dcb80c>\u001b[0m in \u001b[0;36mtrain_and_eval_auc\u001b[0;34m(train_x, train_y, test_x, test_y, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_eval_auc\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cindy/Code/thesis/tf_custom_models/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# One-hot encoding of target `y`, and creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# of a class attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_output_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Build the computation graph. This method is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cindy/Code/thesis/tf_custom_models/base.pyc\u001b[0m in \u001b[0;36mprepare_output_data\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# one liner shortcut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;31m#y = self.onehot_encode(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "for i, field in enumerate(HATEBASE_FIELDS):\n",
    "    print field\n",
    "    tf.reset_default_graph()\n",
    "    train_and_eval_auc( train_x, train_y.iloc[:,i], test_x, test_y.iloc[:,i], OneLayerNN() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = SoftmaxClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 1.18320953846 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Jaccard similarity: 0.869352869353\n"
     ]
    }
   ],
   "source": [
    "total_jaccard( train_x, train_y.iloc[:,:7], test_x, test_y.iloc[:,:7].values, lr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about_class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.245341107249 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.459164292498\n",
      "about_ethnicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.192264601588 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.70749665328\n",
      "about_sexual_orientation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.0931176915765 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.65483234714\n",
      "about_religion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.448324710131 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.570472163495\n",
      "about_disability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.430169701576 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.555096418733\n",
      "about_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.085390098393 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.296768707483\n",
      "about_nationality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 0.0770960450172 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.335260115607\n",
      "offensiveness\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7f3b7a647a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHATEBASE_FIELDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_and_eval_auc\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmaxClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-e847b1dcb80c>\u001b[0m in \u001b[0;36mtrain_and_eval_auc\u001b[0;34m(train_x, train_y, test_x, test_y, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_eval_auc\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cindy/Code/thesis/tf_custom_models/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# One-hot encoding of target `y`, and creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# of a class attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_output_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Build the computation graph. This method is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cindy/Code/thesis/tf_custom_models/base.pyc\u001b[0m in \u001b[0;36mprepare_output_data\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# one liner shortcut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;31m#y = self.onehot_encode(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "for i, field in enumerate(HATEBASE_FIELDS):\n",
    "    print field\n",
    "    train_and_eval_auc( train_x, train_y.iloc[:,i], test_x, test_y.iloc[:,i], SoftmaxClassifier() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: boundary for vectors passed to jaccard sim is 0.5\n",
    "\n",
    "One Layer NN:\n",
    "\n",
    "`\n",
    "On 80-20 split\n",
    "Iteration 1000: loss: 0.0963332206011 \n",
    "Total Jaccard similarity: 0.862026862027\n",
    "`\n",
    "\n",
    "\n",
    "`about_class\n",
    "Iteration 1000: loss: 0.184165582061 \n",
    "AUC: 0.472934472934\n",
    "about_ethnicity\n",
    "Iteration 1000: loss: 0.0597166158259 \n",
    "AUC: 0.564257028112\n",
    "about_sexual_orientation\n",
    "Iteration 1000: loss: 0.0233292710036 \n",
    "AUC: 0.474358974359\n",
    "about_religion\n",
    "Iteration 1000: loss: 0.160992875695 \n",
    "AUC: 0.525017618041\n",
    "about_disability\n",
    "Iteration 1000: loss: 0.168781414628 \n",
    "AUC: 0.554943373125\n",
    "about_gender\n",
    "Iteration 1000: loss: 0.0206541772932 \n",
    "AUC: 0.585034013605\n",
    "about_nationality\n",
    "Iteration 1000: loss: 0.0504829958081 \n",
    "AUC: 0.578034682081\n",
    "`\n",
    "\n",
    "Softmax:\n",
    "\n",
    "`\n",
    "On 80-20 split\n",
    "Iteration 1000: loss: 1.18320953846 \n",
    "Total Jaccard similarity: 0.869352869353\n",
    "`\n",
    "\n",
    "\n",
    "`about_class\n",
    "Iteration 1000: loss: 0.245341107249 \n",
    "AUC: 0.459164292498\n",
    "about_ethnicity\n",
    "Iteration 1000: loss: 0.192264601588 \n",
    "AUC: 0.70749665328\n",
    "about_sexual_orientation\n",
    "Iteration 1000: loss: 0.0931176915765 \n",
    "AUC: 0.65483234714\n",
    "about_religion\n",
    "Iteration 1000: loss: 0.448324710131 \n",
    "AUC: 0.570472163495\n",
    "about_disability\n",
    "Iteration 1000: loss: 0.430169701576 \n",
    "AUC: 0.555096418733\n",
    "about_gender\n",
    "Iteration 1000: loss: 0.085390098393 \n",
    "AUC: 0.296768707483\n",
    "about_nationality\n",
    "Iteration 1000: loss: 0.0770960450172 \n",
    "AUC: 0.335260115607\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
