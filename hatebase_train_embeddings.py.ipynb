{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "\n",
    "from tf_custom_models import OneLayerNN\n",
    "from utility import train_and_eval_auc, HATEBASE_FIELDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# tf.app.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\n",
    "# tf.app.flags.DEFINE_float(\"dropout\", 0.15, \"Fraction of units randomly dropped on non-recurrent connections.\")\n",
    "# tf.app.flags.DEFINE_integer(\"batch_size\", 10, \"Batch size to use during training.\")\n",
    "# tf.app.flags.DEFINE_integer(\"epochs\", 0, \"Number of epochs to train.\")\n",
    "tf.app.flags.DEFINE_integer(\"state_size\", 50, \"Size of hidden layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"embedding_size\", 100, \"Size of the pretrained vocabulary. (default 100)\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"../data/hatebase\", \"Hatebase directory (default ../data/hatebase)\")\n",
    "tf.app.flags.DEFINE_string(\"vocab_path\", \"../data/twitter_davidson/vocab.dat\", \"Path to vocab file (default: ../data/twitter_davidson/vocab.dat)\")\n",
    "tf.app.flags.DEFINE_boolean(\"force_load_embeddings\", False, \"Force loading new hatebase embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfound_i = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embed_path, vocab, force=False):\n",
    "    GLOVE_SIZE = 1193514\n",
    "    GLOVE_PATH = \"../data/glove/glove.twitter.27B.%dd.txt\" % FLAGS.embedding_size\n",
    "\n",
    "    if force or not os.path.exists(embed_path):\n",
    "        hb_vecs = np.zeros((len(vocab), FLAGS.embedding_size))\n",
    "        with open(GLOVE_PATH, 'r') as fh:\n",
    "            found = []\n",
    "            for line in tqdm(fh, total=GLOVE_SIZE):\n",
    "                array = line.strip().split(\" \")\n",
    "                word = array[0]\n",
    "                if word in vocab:\n",
    "                    idx = vocab[word]\n",
    "                    found.append(idx)\n",
    "                    vector = list(map(float, array[1:]))\n",
    "                    hb_vecs[idx, :] = vector\n",
    "            # words not found are set to average of other words\n",
    "            avg = hb_vecs[found, :].mean(axis=0)\n",
    "            unfound = list(set(vocab.values()) - set(found))\n",
    "            hb_vecs[unfound, :] = avg\n",
    "        hb_vecs = pd.DataFrame(hb_vecs)\n",
    "        hb_vecs.to_csv(embed_path, header = False, index = False)\n",
    "        return hb_vecs, unfound\n",
    "\n",
    "    with open(embed_path, 'rb') as embed_path:\n",
    "        data_x = pd.read_csv( embed_path, header = None, quoting = 0, dtype = np.float32 )\n",
    "        return data_x, unfound_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compare_embeddings(original_embeddings, tuned_embeddings, vocab, dimreduce_type=\"pca\", random_state=0):\n",
    "    \"\"\" Compare embeddings drift. \"\"\"\n",
    "    if dimreduce_type == \"pca\":\n",
    "        from sklearn.decomposition import PCA\n",
    "        dimreducer = PCA(n_components=2, random_state=random_state)\n",
    "    elif dimreduce_type == \"tsne\":\n",
    "        from sklearn.manifold import TSNE\n",
    "        dimreducer = TSNE(n_components=2, random_state=random_state)\n",
    "    else:\n",
    "        raise Exception(\"Wrong dimreduce_type.\")\n",
    "\n",
    "    reduced_original = dimreducer.fit_transform(original_embeddings)\n",
    "    reduced_tuned = dimreducer.fit_transform(tuned_embeddings)\n",
    "\n",
    "    def compare_embeddings(word):\n",
    "        if word not in vocab:\n",
    "            return None\n",
    "        word_id = vocab[word]\n",
    "        original_x, original_y = reduced_original[word_id, :]\n",
    "        tuned_x, tuned_y = reduced_tuned[word_id, :]\n",
    "        return original_x, original_y, tuned_x, tuned_y\n",
    "\n",
    "    return compare_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embeddings(embeddings_list, vocab):\n",
    "    '''Takes list of embeddings that have the same indices.\n",
    "    Each set of embeddings will be plotted in a different color.'''\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    colors = itertools.cycle([\"r\", \"b\", \"g\"])\n",
    "\n",
    "    for wv in embeddings_list:\n",
    "        Y = pca.fit_transform(wv)\n",
    "     \n",
    "        plt.scatter(Y[:, 0], Y[:, 1], color=next(colors))\n",
    "        for label, x, y in zip(vocab, Y[:, 0], Y[:, 1]):\n",
    "            plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 613937/1193514 [00:06<00:06, 93985.27it/s]"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "embed_path = pjoin(FLAGS.data_dir, \"embeddings.%dd.vec\") % FLAGS.embedding_size\n",
    "hb_path = pjoin(FLAGS.data_dir, \"lexicon.csv\")\n",
    "\n",
    "hatebase_data = pd.read_csv( hb_path, header = 0, index_col = 0, quoting = 0, \n",
    "                                dtype = HATEBASE_FIELDS, usecols = range(9) )\n",
    "vocab = dict([(x, y) for (y, x) in enumerate(hatebase_data.index)])\n",
    "hatebase_embeddings, unfound_i = load_embeddings(embed_path, vocab, True)\n",
    "\n",
    "train_i, test_i = train_test_split( np.arange( len( hatebase_embeddings )), train_size = 0.9, random_state = 44 )\n",
    "train_x = hatebase_embeddings.ix[train_i]\n",
    "test_x = hatebase_embeddings.ix[test_i]\n",
    "train_y = hatebase_data.ix[train_i]\n",
    "test_y = hatebase_data.ix[test_i]\n",
    "\n",
    "nn = OneLayerNN()\n",
    "#train_and_eval_auc( train_x, train_y, test_x, test_y, model=nn )\n",
    "nn.fit( hatebase_embeddings, hatebase_data )\n",
    "hidden_states = nn.return_hidden_states( hatebase_embeddings )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-94375df4adf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print_embeddings( [hatebase_embeddings.values, hidden_states], vocab, 50 )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munfound_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhatebase_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munfound_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint_embeddings\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhatebase_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munfound_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munfound_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munfound_vocab\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# graph stuff\n",
    "#cmpr_fn = get_compare_embeddings(hatebase_embeddings, hidden_states, vocab)\n",
    "#print_embeddings( [hatebase_embeddings.values, hidden_states], vocab, 50 )\n",
    "unfound_vocab = [hatebase_data.index[i] for i in unfound_i]\n",
    "print_embeddings( [hatebase_embeddings.ix[unfound_i], hidden_states[unfound_i, :]], unfound_vocab )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bluegum', 'neechee', 'jockie', 'russellite ', 'mud shark', 'zionazi', 'kotiya', 'papist', 'paleface', 'pineapple nigger', 'dune nigger', 'stump jumper', 'shyster', 'fuzzy wuzzy', 'darkey', 'towel head', 'honkie', 'bounty bar', 'kushite', 'whore from fife', 'mackerel snapper ', 'nitchee', 'conspiracy theorist', 'black dago', 'sawney', 'moss eater', 'heinie', 'slopehead', 'zippohead', 'gooky', 'rico suave', 'lugan', 'island nigger', 'carpet pilot', 'hymie', 'cocoa puff', 'eight ball', 'timber nigger', 'buddhahead', 'sideways pussy', 'spice nigger', 'smoke jumper', 'hayseed', 'banjo lips', 'eh hole', 'aunt jemima', 'carrot snapper', 'powderburn', 'mangia cake', 'gurrier', 'gator bait', 'dhimmi', 'bitter clinger', 'muzzie', 'shanty irish', 'aunt sally', 'moulie', 'jungle bunny', 'gippo', 'chonky', 'diaper head', 'cowboy killer', 'americoon', 'tynkere', 'scag', 'nig nog', 'proddy dog', 'octroon', 'wexican', 'ching chong', 'charver', 'lowlander', 'clamhead', 'bamboo coon', 'camel cowboy', 'ocker', 'caublasian', 'roundeye', 'charva', 'gyppie', 'brass ankle', 'trailer park trash', 'hairyback', 'mocky', 'ofay', 'hunkie', 'sucker fish', 'white nigger', 'northern monkey', 'sperg', 'ghetto hamster', 'alligator bait', 'african catfish', 'camel jockey', 'mil bag', 'spigger', 'fog nigger', 'orangie', 'muktuk', 'shelta', 'four by two', 'aunt jane', 'bans and cans', 'buckra', 'white chocolate', 'beach nigger', 'net head', 'white trash', 'spigotty', 'curry slurper', 'camel humper', 'velcro head', 'higger', 'bean dipper', 'afro-saxon', 'half breed', 'proddywhoddy', 'semihole', 'dingo fucker', 'smoked irishman', 'blaxican', 'sideways cooter', 'house nigger', 'hunyak', 'dyke jumper', 'booner', 'christ killer', 'bhrempti', 'wagon burner', 'burrhead', 'tynkard', 'tynkare', 'pointy head', 'ling ling', 'shit kicker', 'hebro', 'proddywoddy', 'uncle tom', 'black barbie', 'camel jacker', 'prairie nigger', 'raghead', 'cushite', 'scallie', 'tunnel digger', 'niggress', 'border nigger', 'seppo', 'jiggabo', 'lawn jockey', 'coon ass', 'cuckservative', 'tree jumper', 'ice nigger', 'butterhead', 'buk buk', 'chigger', 'sub human', 'eyetie', 'nigette', 'gin jockey', 'banana lander', 'pommie grant', 'jewbacca', 'zigabo', 'pickaninny', 'plastic paddy', 'zipperhead', 'southern fairy', 'beaney', 'anchor baby', 'closet fag', 'touch of the tar brush', 'polack', 'ikey mo', 'bog jumper', 'slopy', 'moulignon', 'quadroon', 'mud person', 'cotton picker', 'fresh off the boat', 'skanger', 'sheeny', 'surrender monkey', 'hunyock', 'moulinyan', 'mulignan', 'jijjiboo', 'trailer trash', 'jigarooni', 'bix nood', 'whigger', 'tynker', 'porch monkey', 'banana bender', 'chinese wetback', 'slant eye', 'nigor', 'quashie', 'soup taker ', 'ice monkey', 'neejee', 'oven dodger', 'japie', 'cheese eating surrender monkey', 'africoon', 'dot head', 'tar baby', 'sheepfucker', 'yobbo', 'roofucker', 'golliwog', 'nitchy', 'mickey finn', 'sideways vagina', 'spiv', 'rhine monkey', 'race traitor', 'curry stinker', 'pommie', 'moon cricket', 'bog trotter', 'mud duck', 'chi chi', 'lebbo', 'chee chee', 'smoked irish', 'groid', 'mockie', 'beaner shnitzel', 'camel fucker', 'chink a billy', 'curry muncher', 'ginzo', 'tinkard', 'yellow bone', 'wiggerette', 'uncircumcised baboon', 'cow kisser', 'moky', 'tincker', 'spig', 'honyak', 'border hopper', 'niggor', 'honyock', 'tinkar', 'cab nigger', 'nigre', 'junior mint', 'chili shitter', 'pohm', 'border jumper', 'tynekere', 'jhant', 'border bunny', 'ping pang', 'spickaboo', 'spear chucker', 'red bone', 'mockey', 'shit heel', 'pancake face', 'stovepipe', 'boonga', 'closetfag', 'scanger', 'guala guala', 'peckerwood', 'gun burglar', 'dune coon', 'tyncar', 'can eater', 'thicklips', 'book book', 'chinig', 'nitchie', 'lubra', 'gyppo', 'bug eater', 'sheister', 'bog hopper', 'gyppy', 'aunt mary', 'latrino', 'bog irish', 'jim fish', 'dole bludger', 'cave nigger', 'zog lover', 'octaroon', 'dogun', 'squarehead', 'tinkere', 'slopey', 'gook eye', 'mosshead']\n"
     ]
    }
   ],
   "source": [
    "print [hatebase_data.index[i] for i in unfound_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
