{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file tests the one layer NN vs a direct linear classifier using GloVe vectors to predict the 8 hatebase features. We define a new cost function that is similar to AUC but deals with the 8 features separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_custom_models import OneLayerNNRetrofit, SoftmaxClassifier\n",
    "from utility import train_and_eval_auc, HATEBASE_FIELDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/hatebase\"\n",
    "EMBEDDING_SIZE = 100\n",
    "STATE_SIZE = 100\n",
    "GLOVE_SIZE = 1193514\n",
    "GLOVE_PATH = \"data/glove/glove.twitter.27B.%dd.txt\" % EMBEDDING_SIZE\n",
    "\n",
    "EMBED_PATH = \"data/hatebase/embeddings.%dd.dat\" % EMBEDDING_SIZE\n",
    "HIDDEN_EMBED_PATH = \"data/hatebase/embeddings.hidden.%dd.dat\" % EMBEDDING_SIZE\n",
    "NEW_EMBED_PATH = \"data/hatebase/embeddings.new.%dd.dat\" % EMBEDDING_SIZE\n",
    "HB_PATH = \"data/hatebase/lexicon.csv\"\n",
    "VOCAB_PATH = \"data/hatebase/vocab.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embed_path, vocab, force=False):\n",
    "    if not os.path.exists(embed_path):\n",
    "        hb_vecs = np.zeros((len(vocab), EMBEDDING_SIZE))\n",
    "        with open(GLOVE_PATH, 'r') as fh:\n",
    "            found = []\n",
    "            for line in tqdm(fh, total=GLOVE_SIZE):\n",
    "                array = line.strip().split(\" \")\n",
    "                word = array[0]\n",
    "                if word in vocab:\n",
    "                    idx = vocab[word]\n",
    "                    found.append(idx)\n",
    "                    vector = list(map(np.float64, array[1:]))\n",
    "                    hb_vecs[idx, :] = vector\n",
    "            # words not found are set to random values\n",
    "            unfound = list(set(vocab.values()) - set(found))\n",
    "            for i in unfound:\n",
    "                hb_vecs[i, :] = np.random.randn(EMBEDDING_SIZE)\n",
    "                \n",
    "        hb_vecs = pd.DataFrame(hb_vecs)\n",
    "        hb_vecs.to_csv(embed_path, header = False, index = False)\n",
    "        return hb_vecs\n",
    "\n",
    "    with open(embed_path, 'rb') as embed_path:\n",
    "        data_x = pd.read_csv( embed_path, header = None, quoting = 0, dtype = np.float64 )\n",
    "        return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multioutput_y(data_y):\n",
    "    # y transformed from [[0,0,1],[1,0,1]] => [[[1,0],[0,1]],...]\n",
    "    # one hots in pos i of each vec form y_i\n",
    "    y = []\n",
    "    for i in range(len(HATEBASE_FIELDS)):\n",
    "        y.append([np.eye(2)[vec[i]] for vec in data_y.values])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if sys.path[0] == '':\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# grab the data\n",
    "hatebase_data = pd.read_csv( HB_PATH, header = 0, index_col = 0, quoting = 0, \n",
    "                                dtype = HATEBASE_FIELDS, usecols = range(len(HATEBASE_FIELDS)+1) )\n",
    "vocab = dict([(x, y) for (y, x) in enumerate(hatebase_data.index)])\n",
    "hatebase_embeddings = load_embeddings(EMBED_PATH, vocab, True)\n",
    "\n",
    "train_i, test_i = train_test_split( np.arange( len( hatebase_embeddings )), train_size = 0.8, random_state = 44 )\n",
    "#train_x = hatebase_embeddings.ix[train_i].values\n",
    "\n",
    "# need to preserve order because vocab\n",
    "train_x = hatebase_embeddings.values\n",
    "test_x = hatebase_embeddings.ix[test_i].values\n",
    "train_y = get_multioutput_y(hatebase_data)\n",
    "test_y = get_multioutput_y(hatebase_data.ix[test_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_auc( train_x, train_y, test_x, test_y, model ):\n",
    "    model.fit( train_x, train_y )\n",
    "    p = model.predict_proba( test_x )\n",
    "    p = p[:,1] if p.shape[1] > 1 else p[:,0]\n",
    "\n",
    "    auc = AUC( test_y, p )\n",
    "    print \"AUC:\", auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 4000: loss: 1.90733430942 "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "nn = OneLayerNNRetrofit(h=200, max_iter=4000, retrofit_iter=2000)\n",
    "new_embeddings = nn.fit( train_x, train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about_religion AUC: 0.692660550459\n",
      "about_gender AUC: 0.832467532468\n",
      "about_nationality AUC: 0.714285714286\n",
      "about_class AUC: 0.552115987461\n",
      "about_ethnicity AUC: 0.630662020906\n",
      "about_sexual_orientation AUC: 0.666666666667\n",
      "about_disability AUC: 0.724137931034\n",
      "Average AUC: 0.687570914754\n"
     ]
    }
   ],
   "source": [
    "probs = nn.predict_proba( test_x )\n",
    "\n",
    "total_auc = 0\n",
    "for i in range(len(probs)):\n",
    "    # hack to get the positive class\n",
    "    y = [s[1] for s in test_y[i]]\n",
    "    y_pred = [s[1] for s in probs[i]]\n",
    "    auc = AUC( y, y_pred )\n",
    "    print HATEBASE_FIELDS.keys()[i], \"AUC:\", auc\n",
    "    total_auc += auc\n",
    "print \"Average AUC:\", total_auc/len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write hidden states\n",
    "new_embeddings = pd.DataFrame(new_embeddings)\n",
    "new_embeddings.to_csv(NEW_EMBED_PATH, header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 retrofit iters\n",
    "\n",
    "``\n",
    "about_religion AUC: 0.418577981651\n",
    "about_gender AUC: 0.67012987013\n",
    "about_nationality AUC: 0.691071428571\n",
    "about_class AUC: 0.496865203762\n",
    "about_ethnicity AUC: 0.498954703833\n",
    "about_sexual_orientation AUC: 0.631578947368\n",
    "about_disability AUC: 0.448275862069\n",
    "Average AUC: 0.550779142484\n",
    "``\n",
    "\n",
    "2000 retrofit iters\n",
    "\n",
    "``\n",
    "about_religion AUC: 0.43004587156\n",
    "about_gender AUC: 0.555844155844\n",
    "about_nationality AUC: 0.8125\n",
    "about_class AUC: 0.524686520376\n",
    "about_ethnicity AUC: 0.571428571429\n",
    "about_sexual_orientation AUC: 0.570175438596\n",
    "about_disability AUC: 0.534482758621\n",
    "Average AUC: 0.571309045204\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_jaccard( train_x, train_y, test_x, test_y, model ):\n",
    "    model.fit( train_x, train_y )\n",
    "    p = model.predict( test_x )\n",
    "    #print p\n",
    "    p = (p >= 0.5).astype(np.float64)\n",
    "    total = sum([jaccard_similarity_score(y_true, y_pred) for y_true, y_pred in zip(test_y, p)])\n",
    "    print \"Total Jaccard similarity:\", total/len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH, mode=\"wb\") as vocab_file:\n",
    "    for w in hatebase_data.index.values:\n",
    "        vocab_file.write(w + b\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "nn = OneLayerNN()\n",
    "total_jaccard( train_x, train_y.iloc[:,:7], test_x, test_y.iloc[:,:7].values, nn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, field in enumerate(HATEBASE_FIELDS):\n",
    "    print field\n",
    "    tf.reset_default_graph()\n",
    "    train_and_eval_auc( train_x, train_y.iloc[:,i], test_x, test_y.iloc[:,i], OneLayerNN() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = SoftmaxClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_jaccard( train_x, train_y.iloc[:,:7], test_x, test_y.iloc[:,:7].values, lr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, field in enumerate(HATEBASE_FIELDS):\n",
    "    print field\n",
    "    train_and_eval_auc( train_x, train_y.iloc[:,i], test_x, test_y.iloc[:,i], SoftmaxClassifier() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: boundary for vectors passed to jaccard sim is 0.5\n",
    "\n",
    "One Layer NN:\n",
    "\n",
    "`\n",
    "On 80-20 split\n",
    "Iteration 1000: loss: 0.0963332206011 \n",
    "Total Jaccard similarity: 0.862026862027\n",
    "`\n",
    "\n",
    "\n",
    "`about_class\n",
    "Iteration 1000: loss: 0.184165582061 \n",
    "AUC: 0.472934472934\n",
    "about_ethnicity\n",
    "Iteration 1000: loss: 0.0597166158259 \n",
    "AUC: 0.564257028112\n",
    "about_sexual_orientation\n",
    "Iteration 1000: loss: 0.0233292710036 \n",
    "AUC: 0.474358974359\n",
    "about_religion\n",
    "Iteration 1000: loss: 0.160992875695 \n",
    "AUC: 0.525017618041\n",
    "about_disability\n",
    "Iteration 1000: loss: 0.168781414628 \n",
    "AUC: 0.554943373125\n",
    "about_gender\n",
    "Iteration 1000: loss: 0.0206541772932 \n",
    "AUC: 0.585034013605\n",
    "about_nationality\n",
    "Iteration 1000: loss: 0.0504829958081 \n",
    "AUC: 0.578034682081\n",
    "`\n",
    "\n",
    "Softmax:\n",
    "\n",
    "`\n",
    "On 80-20 split\n",
    "Iteration 1000: loss: 1.18320953846 \n",
    "Total Jaccard similarity: 0.869352869353\n",
    "`\n",
    "\n",
    "\n",
    "`about_class\n",
    "Iteration 1000: loss: 0.245341107249 \n",
    "AUC: 0.459164292498\n",
    "about_ethnicity\n",
    "Iteration 1000: loss: 0.192264601588 \n",
    "AUC: 0.70749665328\n",
    "about_sexual_orientation\n",
    "Iteration 1000: loss: 0.0931176915765 \n",
    "AUC: 0.65483234714\n",
    "about_religion\n",
    "Iteration 1000: loss: 0.448324710131 \n",
    "AUC: 0.570472163495\n",
    "about_disability\n",
    "Iteration 1000: loss: 0.430169701576 \n",
    "AUC: 0.555096418733\n",
    "about_gender\n",
    "Iteration 1000: loss: 0.085390098393 \n",
    "AUC: 0.296768707483\n",
    "about_nationality\n",
    "Iteration 1000: loss: 0.0770960450172 \n",
    "AUC: 0.335260115607\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
